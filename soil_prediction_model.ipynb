{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kienvillegas/SnapSoil/blob/master/soil_prediction_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_2nx9I-0aqp",
        "outputId": "48e9bf0d-9451-4b7c-fefe-f40def45d15f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "tXhfcAp32xdO",
        "outputId": "69695b66-abdd-49a6-a3cb-9817b2e86fec"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Length of values (710) does not match length of index (770)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-972e87357a3e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Add image filenames as 'id' in the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Print initial dataset size and image count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4310\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4311\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4522\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4523\u001b[0m         \"\"\"\n\u001b[0;32m-> 4524\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4526\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5266\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5267\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5268\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \"\"\"\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    574\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (710) does not match length of index (770)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Paths to dataset and images\n",
        "img_path = '/content/drive/MyDrive/Thesis_Dataset/soil_images'\n",
        "csv_path = '/content/drive/MyDrive/Thesis_Dataset/sensor_data.csv'\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# List of image files (only .jpg)\n",
        "image_files = [f for f in os.listdir(img_path) if f.endswith(\".jpg\")]\n",
        "image_files.sort()\n",
        "\n",
        "# Add image filenames as 'id' in the dataframe\n",
        "df['id'] = image_files\n",
        "\n",
        "# Print initial dataset size and image count\n",
        "num_rows = len(df)\n",
        "num_images_before = len(image_files)\n",
        "print(f\"Number of rows before cleanup: {num_rows}\")\n",
        "print(f\"Number of images before cleanup: {num_images_before}\")\n",
        "\n",
        "# Filter out rows with 0 or NaN values in N, P, K, or pH\n",
        "df_invalid = df[df[['N', 'P', 'K', 'pH']].eq(0).any(axis=1) | df[['N', 'P', 'K', 'pH']].isna().any(axis=1)]\n",
        "\n",
        "# Get image IDs of invalid records\n",
        "image_ids_to_delete = df_invalid['id'].tolist()\n",
        "\n",
        "# Delete corresponding images from the filesystem\n",
        "for image_id in image_ids_to_delete:\n",
        "    image_file_path = os.path.join(img_path, image_id)\n",
        "    if os.path.exists(image_file_path):\n",
        "        os.remove(image_file_path)\n",
        "        print(f\"Deleted image file: {image_file_path}\")\n",
        "    else:\n",
        "        print(f\"Image file not found: {image_file_path}\")\n",
        "\n",
        "# Now remove these invalid records from the dataframe\n",
        "df = df[~df['id'].isin(image_ids_to_delete)]\n",
        "\n",
        "# Save the updated DataFrame back to the CSV\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "\n",
        "# Verify the number of images remaining\n",
        "image_files_after = [f for f in os.listdir(img_path) if f.endswith(\".jpg\")]\n",
        "num_images_after = len(image_files_after)\n",
        "\n",
        "# Print results\n",
        "print(f\"Number of rows after cleanup: {len(df)}\")\n",
        "print(f\"Number of images after cleanup: {num_images_after}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size if needed\n",
        "for column in ['N', 'P', 'K', 'pH']:\n",
        "    plt.subplot(2, 2, df.columns.get_loc(column) + 1)  # Create subplots\n",
        "    sns.histplot(df[column], kde=True)  # Plot histogram with kernel density estimation\n",
        "    plt.title(f'Distribution of {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()  # Adjust subplot spacing\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "UbA41UYSPIIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYMNbeGqJFs5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.applications import ResNet50, VGG16\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg_preprocess\n",
        "from sklearn.model_selection import train_test_split\n",
        "import xgboost as xgb\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "def create_cnn_model(input_shape):\n",
        "  model = Sequential([\n",
        "      Conv2D(32, (3,3), activation='relu', input_shape=input_shape),\n",
        "      MaxPooling2D((2,2)),\n",
        "      Conv2D(64, (3,3), activation='relu',),\n",
        "      MaxPooling2D((2,2)),\n",
        "      Conv2D(128, (3,3), activation='relu'),\n",
        "      GlobalAveragePooling2D()\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def extract_features(img_path, model, preprocess_input):\n",
        "  img = image.load_img(img_path, target_size=(224,224))\n",
        "  img_data = image.img_to_array(img)\n",
        "  img_data = np.expand_dims(img_data, axis=0)\n",
        "  if preprocess_input :\n",
        "    img_data = preprocess_input(img_data)\n",
        "  else :\n",
        "    img_data = img_data / 255.0\n",
        "\n",
        "  # datagen = ImageDataGenerator(\n",
        "  #     rotation_range = 20,\n",
        "  #     width_shift_range = 0.2,\n",
        "  #     height_shift_range = 0.2,\n",
        "  #     shear_range = 0.2,\n",
        "  #     zoom_range = 0.2,\n",
        "  #     horizontal_flip = True,\n",
        "  #     fill_mode = 'nearest'\n",
        "  # )\n",
        "\n",
        "  # aug_img_data = next(datagen.flow(img_data, batch_size=1))\n",
        "\n",
        "  features = model.predict(img_data)\n",
        "  return features\n",
        "\n",
        "def evaluate_model(model_name, X_train, X_test, y_train, y_test):\n",
        "    if model_name == \"XGBoost\":\n",
        "        model_n = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "        model_p = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "        model_k = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "        model_pH = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
        "    elif model_name == \"RandomForest\":\n",
        "        model_n = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        model_p = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        model_k = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "        model_pH = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    elif model_name == \"GBM\":\n",
        "        model_n = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "        model_p = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "        model_k = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "        model_pH = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "    else:\n",
        "        print(\"Invalid Model Name!\")\n",
        "\n",
        "    model_n.fit(X_train, y_train[:,0])\n",
        "    model_p.fit(X_train, y_train[:,1])\n",
        "    model_k.fit(X_train, y_train[:,2])\n",
        "    model_pH.fit(X_train, y_train[:,3])\n",
        "\n",
        "    y_pred_n = model_n.predict(X_test)\n",
        "    y_pred_p = model_p.predict(X_test)\n",
        "    y_pred_k = model_k.predict(X_test)\n",
        "    y_pred_pH = model_pH.predict(X_test)\n",
        "\n",
        "    mse_n = mean_squared_error(y_test[:,0], y_pred_n)\n",
        "    mse_p = mean_squared_error(y_test[:,1], y_pred_p)\n",
        "    mse_k = mean_squared_error(y_test[:,2], y_pred_k)\n",
        "    mse_pH = mean_squared_error(y_test[:,3], y_pred_pH)\n",
        "\n",
        "    rmse_n = np.sqrt(mse_n)\n",
        "    rmse_p = np.sqrt(mse_p)\n",
        "    rmse_k = np.sqrt(mse_k)\n",
        "    rmse_pH = np.sqrt(mse_pH)\n",
        "\n",
        "    mae_n = mean_absolute_error(y_test[:,0], y_pred_n)\n",
        "    mae_p = mean_absolute_error(y_test[:,1], y_pred_p)\n",
        "    mae_k = mean_absolute_error(y_test[:,2], y_pred_k)\n",
        "    mae_pH = mean_absolute_error(y_test[:,3], y_pred_pH)\n",
        "\n",
        "    def calculate_mpe(y_test, y_pred):\n",
        "      errors = y_test - y_pred\n",
        "      absolute_errors = np.abs(errors)\n",
        "      relative_errors = absolute_errors / np.maximum(np.abs(y_test), 1e-6)\n",
        "      mpe = np.mean(relative_errors) * 100\n",
        "      return mpe\n",
        "\n",
        "\n",
        "    mpe_n = calculate_mpe(y_test[:,0], y_pred_n)\n",
        "    mpe_p = calculate_mpe(y_test[:,1], y_pred_p)\n",
        "    mpe_k = calculate_mpe(y_test[:,2], y_pred_k)\n",
        "    mpe_pH = calculate_mpe(y_test[:,3], y_pred_pH)\n",
        "\n",
        "    r2_n = r2_score(y_test[:,0], y_pred_n)\n",
        "    r2_p = r2_score(y_test[:,1], y_pred_p)\n",
        "    r2_k = r2_score(y_test[:,2], y_pred_k)\n",
        "    r2_pH = r2_score(y_test[:,3], y_pred_pH)\n",
        "\n",
        "    def adjusted_r2_score(r2, n, p):\n",
        "        return 1 - ((1 - r2) * ((n - 1) / (n - p - 1)))\n",
        "\n",
        "    adj_r2_n = adjusted_r2_score(r2_n, len(y_test[:,0]), X_test.shape[1])\n",
        "    adj_r2_p = adjusted_r2_score(r2_p, len(y_test[:,1]), X_test.shape[1])\n",
        "    adj_r2_k = adjusted_r2_score(r2_k, len(y_test[:,2]), X_test.shape[1])\n",
        "    adj_r2_pH = adjusted_r2_score(r2_pH, len(y_test[:,3]), X_test.shape[1])\n",
        "\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"RMSE (N): {rmse_n}\")\n",
        "    print(f\"RMSE (P): {rmse_p}\")\n",
        "    print(f\"RMSE (K): {rmse_k}\")\n",
        "    print(f\"RMSE (pH): {rmse_pH}\")\n",
        "    print()\n",
        "    print(f\"MAE (N): {mae_n}\")\n",
        "    print(f\"MAE (P): {mae_p}\")\n",
        "    print(f\"MAE (K): {mae_k}\")\n",
        "    print(f\"MAE (pH): {mae_pH}\")\n",
        "    print()\n",
        "    print(f\"MPE (N): {mpe_n}\")\n",
        "    print(f\"MPE (P): {mpe_p}\")\n",
        "    print(f\"MPE (K): {mpe_k}\")\n",
        "    print(f\"MPE (pH): {mpe_pH}\")\n",
        "    print()\n",
        "    print(f\"Adjusted R-squared (N): {adj_r2_n}\")\n",
        "    print(f\"Adjusted R-squared (P): {adj_r2_p}\")\n",
        "    print(f\"Adjusted R-squared (K): {adj_r2_k}\")\n",
        "    print(f\"Adjusted R-squared (pH): {adj_r2_pH}\")\n",
        "    print()\n",
        "\n",
        "resnet_base_model = ResNet50(weights='imagenet',include_top=False, pooling='avg')\n",
        "vgg16_base_model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
        "cnn_model = create_cnn_model((224, 224, 3))\n",
        "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "\n",
        "resnet_features_list = []\n",
        "vgg16_features_list = []\n",
        "cnn_features_list = []\n",
        "\n",
        "for filename in os.listdir(img_path):\n",
        "  if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "    full_image_path = os.path.join(img_path, filename)\n",
        "\n",
        "    resnet_features = extract_features(full_image_path, resnet_base_model, resnet_preprocess)\n",
        "    resnet_features_list.append(resnet_features)\n",
        "\n",
        "    vgg16_features = extract_features(full_image_path, vgg16_base_model, vgg_preprocess)\n",
        "    vgg16_features_list.append(vgg16_features)\n",
        "\n",
        "    cnn_features = extract_features(full_image_path, cnn_model,None)\n",
        "    cnn_features_list.append(cnn_features)\n",
        "\n",
        "resnet_features_array = np.vstack(resnet_features_list)\n",
        "vgg16_features_array = np.vstack(vgg16_features_list)\n",
        "cnn_features_array = np.vstack(cnn_features_list)\n",
        "\n",
        "y = df[['N', 'P', 'K', 'pH']].values\n",
        "print(f\"Shape of y: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "msKxRma4MhW9"
      },
      "outputs": [],
      "source": [
        "resnet_base_model.save('resnet_model.h5')\n",
        "vgg16_base_model.save('vgg16_model.h5')\n",
        "cnn_model.save('cnn_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJnCyQgBMxW1"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "for features_array, model_name in [\n",
        "    (resnet_features_array, \"ResNet50\"),\n",
        "    (vgg16_features_array, \"VGG16\"),\n",
        "    (cnn_features_array, \"CNN\")\n",
        "]:\n",
        "    X = features_array.reshape(features_array.shape[0], -1)\n",
        "    print(f\"Shape of {model_name} features: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape}\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    for reg_model in [\"XGBoost\", \"RandomForest\", \"GBM\"]:\n",
        "        evaluate_model(reg_model, X_train, X_test, y_train, y_test)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+Yut9gdaH7S4HEsynhUBE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}